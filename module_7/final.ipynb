{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект Ford vs. Ferrari определение марки авто по изображению\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Цель: используя предобученные сети и различные техники обучения, построить модель, классифицирующую автомобиль по его изображению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для достижения поставленной цели были произведены эксперименты, которые включали в себя вариации следующих параметров:\n",
    "\n",
    "* learning rate = {1e-2, 1e-3, 1e-4}\n",
    "* image size = {180, 224, 280}\n",
    "* augmentation = {base, advanced}\n",
    "* net = {Xception, EfficientNetB7}\n",
    "* head = {base simple, base, base+batch}\n",
    "* finetuning = {yes, no}\n",
    "\n",
    "Ниже преведен код, который показал наилучший результат в соревновании Kaggle. Он содержит описание всех действий, а также некоторые выводы, которые удалось сделать в процессе экспериментов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим efficientnet и библиотеку для продвинутой аугментации изображений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install - q efficientnet\n",
    "!pip install git+https: // github.com/mjkvaak/ImageDataAugmentor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем необходимые библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import zipfile\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "import efficientnet.tfkeras as efn\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.callbacks as C\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from ImageDataAugmentor.image_data_augmentor import *\n",
    "import albumentations\n",
    "\n",
    "import PIL\n",
    "from PIL import ImageOps, ImageFilter\n",
    "# увеличим дефолтный размер графиков\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "# графики в svg выглядят более четкими\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "\n",
    "print(os.listdir(\"../input\"))\n",
    "print('Python       :', sys.version.split('\\n')[0])\n",
    "print('Numpy        :', np.__version__)\n",
    "print('Tensorflow   :', tf.__version__)\n",
    "print('Keras        :', tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим наличие доступа к видеокарточкам и установим необходимые программы из requierements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основные настройки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В setup выносим основные настройки: так удобнее их перебирать в дальнейшем.\n",
    "\n",
    "EPOCHS = 10  # эпох на обучение\n",
    "BATCH_SIZE = 32  # уменьшаем batch если сеть большая, иначе не влезет в память на GPU\n",
    "LR = 1e-3\n",
    "VAL_SPLIT = 0.2  # сколько данных выделяем на тест = 20%\n",
    "\n",
    "CLASS_NUM = 10  # количество классов в нашей задаче\n",
    "IMG_SIZE = 280  # какого размера подаем изображения в сеть (180, 224)\n",
    "IMG_CHANNELS = 3   # у RGB 3 канала\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)\n",
    "\n",
    "DATA_PATH = '../input/'\n",
    "PATH = \"../working/car/\"  # рабочая директория\n",
    "\n",
    "# Устанавливаем random seed для воспроизводимости\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "PYTHONHASHSEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA / Анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_PATH+\"train.csv\")\n",
    "sample_submission = pd.read_csv(DATA_PATH+\"sample-submission.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на распределение классов - оно достаточно равномерное\n",
    "train_df.Category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Распаковываем картинки')\n",
    "# Will unzip the files so that you can see them..\n",
    "for data_zip in ['train.zip', 'test.zip']:\n",
    "    with zipfile.ZipFile(\"../input/\"+data_zip, \"r\") as z:\n",
    "        z.extractall(PATH)\n",
    "print(os.listdir(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "print('Пример картинок (random sample)')\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "random_image = train_df.sample(n=9)\n",
    "random_image_paths = random_image['Id'].values\n",
    "random_image_cat = random_image['Category'].values\n",
    "\n",
    "for index, path in enumerate(random_image_paths):\n",
    "    im = PIL.Image.open(PATH+f'train/{random_image_cat[index]}/{path}')\n",
    "    plt.subplot(3, 3, index+1)\n",
    "    plt.imshow(im)\n",
    "    plt.title('Class: '+str(random_image_cat[index]))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно видеть - классы это модели автомобилей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на примеры картинок и их размеры, чтобы понимать, как их лучше обработать и сжимать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = PIL.Image.open(PATH+'/train/0/100380.jpg')\n",
    "imgplot = plt.imshow(image)\n",
    "plt.show()\n",
    "image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аугментация данных необходима для того, чтобы иметь возможность качественно обучить сеть при наличии небольшого набора обучающих данных. Аугментация - это процесс применения различных преобразований (сжатие, поворот, отражение и др.) к изображениям. В данной работе применялись два типа аугментаций: base (из бейзлайна) и advanced (библиотека albumentations). Наилучший результат был получен с использованием продвинутой техники аугментации. Были использованы преобразования:\n",
    "* горизонтальное отражение\n",
    "* поворот\n",
    "* изменение яркости и контраста\n",
    "* обрезка\n",
    "* наложение Гауссовского фильтра\n",
    "* изменение оттенка и насыщенности\n",
    "* сдвиг значения цветов пикселей\n",
    "* применение метода главных компонент\n",
    "* изменение размера изображения\n",
    "\n",
    "Эти преобразования относятся лишь к тренировочным данным, так как тестирование проводится на изображениях, максимально похожих на входные данные, которые встретятся на практике (без аугментации). Кроме этого, значения яркости пикселей всех имеющихся изображений нормируется. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "# train_datagen = ImageDataGenerator(rescale=1./ 255,\n",
    "#                                    rotation_range = 15,\n",
    "#                                    shear_range=0.2,\n",
    "#                                    zoom_range=[0.75,1.25],\n",
    "#                                    brightness_range=[0.5, 1.5],\n",
    "#                                    width_shift_range=0.1,\n",
    "#                                    height_shift_range=0.1,\n",
    "#                                    validation_split=VAL_SPLIT, # set validation split\n",
    "#                                    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# advanced\n",
    "augmentation = albumentations.Compose([\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.Rotate(limit=20, interpolation=1, border_mode=4,\n",
    "                          value=None, mask_value=None, always_apply=False, p=0.5),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.CenterCrop(height=224, width=200),\n",
    "        albumentations.CenterCrop(height=200, width=224),\n",
    "    ], p=0.5),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.RandomBrightnessContrast(\n",
    "            brightness_limit=0.3, contrast_limit=0.3),\n",
    "        albumentations.RandomBrightnessContrast(\n",
    "            brightness_limit=0.1, contrast_limit=0.1)\n",
    "    ], p=0.5),\n",
    "    albumentations.GaussianBlur(p=0.05),\n",
    "    albumentations.HueSaturationValue(p=0.5),\n",
    "    albumentations.RGBShift(p=0.5),\n",
    "    albumentations.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n",
    "    albumentations.Resize(IMG_SIZE, IMG_SIZE)\n",
    "])\n",
    "\n",
    "train_datagen = ImageDataAugmentor(\n",
    "    rescale=1./255,\n",
    "    augment=augmentation,\n",
    "    validation_split=VAL_SPLIT,\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataAugmentor(rescale=1./255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Поместим наши данные в генератор:\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',      # директория где расположены папки с картинками\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='training')  # пометим как тренировочные\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='validation')  # пометим как тестовые"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве базовой сети были рассмотрены два варианта: Xception и EfficientNetB7. Проведенные эксперименты показали, что лучшее качество достигается при использовании EfficientNetB7. Поэтому загрузим её:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_model = Xception(weights='imagenet', include_top=False, input_shape = input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = efn.EfficientNetB7(\n",
    "    weights='imagenet', include_top=False, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на устройство базовой модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как показали эксперименты, поэтапное обучение модели, использующее \"заморозку\" слоев, дает лучшее качество. Это связано с тем, что базовая модель, обученная на imagenet, подстраивается под наши данные постепенно. То есть сначала мы обучаем лишь \"голову\", оставляя веса базовой модели неизменными, а затем постепенно включаем в обучение слои базовой модели. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# зафиксируем базовую модель\n",
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В процессе построения модели были использованы три модификации \"головы\": base simple, base, base+batch\n",
    "\n",
    "* base simple = (GlobalAveragePooling2D + Dense)\n",
    "* base = (GlobalAveragePooling2D + Dense + Dropout + Dense)\n",
    "* base+batch = (GlobalAveragePooling2D + Dense + BatchNormalization + Dropout + Dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эксперименты показали, что base simple приводит к скорому переобучению модели, хотя и показывает лучший результат на тесте. base и base+batch содержат инструменты для предотвращения переобучения: dropout и batch normalization слои. Лучший результат был получен с использованием батч-нормализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# установка \"головы\"\n",
    "model = M.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(L.GlobalAveragePooling2D())\n",
    "model.add(L.Dense(256, activation='relu'))\n",
    "model.add(L.BatchNormalization())\n",
    "model.add(L.Dropout(0.25))\n",
    "model.add(L.Dense(CLASS_NUM, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно видеть, что базовая модель \"заморожена\", а обучаются только слои \"головы\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение \"головы\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим ModelCheckpoint и EarlyStopping, для того, чтобы иметь возможность сохранять прогресс обучения модели и не совершать лишних вычислений в случае, когда улучшение метрики не происходит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])\n",
    "checkpoint = ModelCheckpoint('best_model.hdf5', monitor=[\n",
    "                             'val_accuracy'], verbose=1, mode='max')\n",
    "earlystop = EarlyStopping(\n",
    "    monitor='accuracy', patience=5, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint, earlystop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим обучение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples//test_generator.batch_size,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на значение метрики для тестовых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(test_generator, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Только при обучении \"головы\" удалось добиться 75% точности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_curves():\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(len(acc))\n",
    "    plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним сеть и загрузим веса лучшего варианта\n",
    "model.save('../working/step1.hdf5')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_kg_hide-input": true
   },
   "source": [
    "### Разморозка (первый шаг)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "разморозим половину слоев базовой модели и продолжим обучение "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "fine_tune = len(base_model.layers)//2\n",
    "for layer in base_model.layers[:fine_tune]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base_model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the trainable status of the individual layers\n",
    "for layer in model.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно видеть, что базовая модель тоже обучается. Продолжим обучение, при этом уменьшим learning rate до 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples//test_generator.batch_size,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на значение метрики на тестовых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(test_generator, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже лучше - 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним сеть и загрузим веса лучшего варианта\n",
    "model.save('../working/step2.hdf5')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разморозка (второй шаг)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разморозим всю сеть, предоставив ей возможность обучаться полностью, при этом снова уменьшим learning rate до 1e-5 и batch_size, чтобы сеть поместилась на карту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "LR = 1e-5\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',      # директория где расположены папки с картинками\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='training')  # пометим как тренировочные\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='validation')  # пометим как тестовые"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])\n",
    "checkpoint = ModelCheckpoint('best_model.hdf5', monitor=[\n",
    "                             'accuracy'], verbose=1, mode='max')\n",
    "earlystop = EarlyStopping(\n",
    "    monitor='accuracy', patience=5, restore_best_weights=True)\n",
    "callbacks_list = [checkpoint, earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples//test_generator.batch_size,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на значение метрики на тестовых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate_generator(test_generator, verbose=1)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат при разморозке всей модели - 97.3%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним сеть и загрузим веса лучшего варианта\n",
    "model.save('../working/step3.hdf5')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дополнительно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из приемов улучшения качества модели является увеличение размера изображения совместно с понижением количества преобразований при аугментации. Воспользуемся этим приемом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 2\n",
    "LR = 1e-5\n",
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (IMG_SIZE, IMG_SIZE, IMG_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = albumentations.Compose([\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.Rotate(limit=20, interpolation=1, border_mode=4,\n",
    "                          value=None, mask_value=None, always_apply=False, p=0.5)\n",
    "])\n",
    "\n",
    "train_datagen = ImageDataAugmentor(\n",
    "    rescale=1./255,\n",
    "    augment=augmentation,\n",
    "    validation_split=VAL_SPLIT,\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataAugmentor(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='training')\n",
    "\n",
    "test_generator = train_datagen.flow_from_directory(\n",
    "    PATH+'train/',\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True, seed=RANDOM_SEED,\n",
    "    subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как размер входных данных изменился, то создадим сеть заново, при этом подгрузим веса, полученные ранее:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = efn.EfficientNetB7(\n",
    "    weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "# установка \"головы\"\n",
    "model = M.Sequential()\n",
    "model.add(base_model)\n",
    "model.add(L.GlobalAveragePooling2D())\n",
    "model.add(L.Dense(256, activation='relu'))\n",
    "model.add(L.BatchNormalization())\n",
    "model.add(L.Dropout(0.25))\n",
    "model.add(L.Dense(CLASS_NUM, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=optimizers.Adam(lr=LR), metrics=[\"accuracy\"])\n",
    "model.load_weights('best_model.hdf5')\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples//train_generator.batch_size,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples//test_generator.batch_size,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../working/step4.hdf5')\n",
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сабмит на Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся итоговой моделью для предсказания класса автомобиля и сделаем сабмит на Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=sample_submission,\n",
    "    directory=PATH+'test_upload/',\n",
    "    x_col=\"Id\",\n",
    "    y_col=None,\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    seed=RANDOM_SEED,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub_generator.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub_generator.reset()\n",
    "predictions = model.predict_generator(\n",
    "    test_sub_generator, steps=len(test_sub_generator), verbose=1)\n",
    "predictions = np.argmax(predictions, axis=-1)  # multiple categories\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v, k) for k, v in label_map.items())  # flip k,v\n",
    "predictions = [label_map[k] for k in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_with_dir = test_sub_generator.filenames\n",
    "submission = pd.DataFrame(\n",
    "    {'Id': filenames_with_dir, 'Category': predictions}, columns=['Id', 'Category'])\n",
    "submission['Id'] = submission['Id'].replace('test_upload/', '')\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Save submit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученный результат на Kaggle - **0.96374**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Time Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одной техникой улучшения модели является Test Time Augmentation. Для этого мы аугментируем тестовые изображения и делаем несколько предсказаний. Затем берем среднее от полученных предсказаний, что и будет финальным ответом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('best_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = albumentations.Compose([\n",
    "    albumentations.HorizontalFlip(p=0.5),\n",
    "    albumentations.Rotate(limit=20, interpolation=1, border_mode=4,\n",
    "                          value=None, mask_value=None, always_apply=False, p=0.5),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.CenterCrop(height=220, width=200),\n",
    "        albumentations.CenterCrop(height=200, width=220),\n",
    "    ], p=0.5),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.RandomBrightnessContrast(\n",
    "            brightness_limit=0.3, contrast_limit=0.3),\n",
    "        albumentations.RandomBrightnessContrast(\n",
    "            brightness_limit=0.1, contrast_limit=0.1)\n",
    "    ], p=0.5),\n",
    "    albumentations.GaussianBlur(p=0.05),\n",
    "    albumentations.HueSaturationValue(p=0.5),\n",
    "    albumentations.RGBShift(p=0.5),\n",
    "    albumentations.FancyPCA(alpha=0.1, always_apply=False, p=0.5),\n",
    "    albumentations.Resize(IMG_SIZE, IMG_SIZE)\n",
    "])\n",
    "\n",
    "test_datagen = ImageDataAugmentor(\n",
    "    rescale=1./255,\n",
    "    augment=augmentation,\n",
    "    validation_split=VAL_SPLIT,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sub_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=sample_submission,\n",
    "    directory=PATH+'test_upload/',\n",
    "    x_col=\"Id\",\n",
    "    y_col=None,\n",
    "    shuffle=False,\n",
    "    class_mode=None,\n",
    "    seed=RANDOM_SEED,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_steps = 10\n",
    "predictions = []\n",
    "\n",
    "for i in range(tta_steps):\n",
    "    preds = model.predict_generator(\n",
    "        test_sub_generator, steps=len(test_sub_generator), verbose=1)\n",
    "    predictions.append(preds)\n",
    "\n",
    "pred = np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax(pred, axis=-1)  # multiple categories\n",
    "label_map = (train_generator.class_indices)\n",
    "label_map = dict((v, k) for k, v in label_map.items())  # flip k,v\n",
    "predictions = [label_map[k] for k in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_with_dir = test_sub_generator.filenames\n",
    "submission = pd.DataFrame(\n",
    "    {'Id': filenames_with_dir, 'Category': predictions}, columns=['Id', 'Category'])\n",
    "submission['Id'] = submission['Id'].replace('test_upload/', '')\n",
    "submission.to_csv('submission_TTA.csv', index=False)\n",
    "print('Save submit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Clean PATH\n",
    "import shutil\n",
    "shutil.rmtree(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TTA дал незначительный прирост качества модели на Kaggle - **0.96958**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В ходе выполнения проекта удалось достичь значение метрики **0.96958**, что является несомненным улучшением baseline.\n",
    "\n",
    "Применены следующие техники:\n",
    "\n",
    "* transfer learning\n",
    "* fine tuning\n",
    "* TTA\n",
    "\n",
    "Опробованы сети для классификации изображений:\n",
    "\n",
    "* Exception \n",
    "* EfficientNetB7\n",
    "\n",
    "Подобраны параметры:\n",
    "\n",
    "* learning rate\n",
    "* image size\n",
    "\n",
    "Подобрана архитектура \"головы\" и использованы дополнительные callback функции в Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
